{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from glob import glob\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "import subprocess\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67c996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils #Drawing utilities\n",
    "mp_holistic = mp.solutions.holistic   #Holistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba56e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width, frame_height = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75667cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frame_per_video = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f9fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WLASL/start_kit/WLASL_v0.3.json\", \"r\") as file:\n",
    "    content = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71398ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b8c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmark(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f64f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmark(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe7bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ae2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_keypoints():\n",
    "    pose = np.zeros(132)\n",
    "    face = np.zeros(1404)\n",
    "    lh = np.zeros(21*3)\n",
    "    rh = np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e6d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820fbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirbyname(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except:\n",
    "        shutil.rmtree(path)\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43ae48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_percent=0.90):\n",
    "    train_sequences, train_labels = [], []\n",
    "    test_sequences, test_labels = [], []\n",
    "    words = glob(\"Data_Numpy/*\")\n",
    "    for word in words:\n",
    "        actual_label = word.split(\"/\")[-1]\n",
    "        videos = glob(word + \"/*\")\n",
    "        #print(f\"-------------{actual_label}-------\")\n",
    "        train_proportion = math.ceil(round(train_percent * len(videos)))\n",
    "        for index,video_ in enumerate(videos):\n",
    "            #print(f\"********---> Video   {video_} ----- \")\n",
    "            window = []\n",
    "            for frame_num in range(1,num_frame_per_video+1):\n",
    "                np_file = f\"{video_}/{frame_num}.npy\"\n",
    "                #print(np_file)\n",
    "                res = np.load(np_file)\n",
    "                window.append(res)\n",
    "            if index <= train_proportion:\n",
    "                train_sequences.append(window)\n",
    "                train_labels.append(label_map[actual_label])\n",
    "            else:\n",
    "                test_sequences.append(window)\n",
    "                test_labels.append(label_map[actual_label])\n",
    "                \n",
    "    train_sequences = np.array(train_sequences)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_sequences = np.array(test_sequences)\n",
    "    test_labels = np.array(test_labels)\n",
    "                \n",
    "    return train_sequences, train_labels, test_sequences, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e97789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_frame(filename):\n",
    "    video_frame_count = 0\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(filename)\n",
    "        video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return video_frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32daea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1b68d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders_video(content):\n",
    "    for i in content:\n",
    "        folder_path = f\"Data/{i['gloss']}\"\n",
    "        createFolder(folder_path)\n",
    "        for j in i['instances']:\n",
    "            if os.path.exists(f\"WLASL/start_kit/videos/{j['video_id']}.mp4\"):\n",
    "                shutil.copy(f\"WLASL/start_kit/videos/{j['video_id']}.mp4\", f\"Data/{i['gloss']}/{j['video_id']}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e14c0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(content):\n",
    "    actions = []\n",
    "    for i in content:\n",
    "        actions.append(i['gloss'])\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b1991cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_blank = blank_keypoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "872290cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_blank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67f4db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = get_actions(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31778b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ef48bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60757b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e71d3",
   "metadata": {},
   "source": [
    "## Generate numpy data for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fccac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = glob(\"Data/*\")\n",
    "makedirbyname(\"Data_Numpy\")\n",
    "for word in words:\n",
    "    videos = glob(word + \"/*\")\n",
    "    for video in videos:\n",
    "        video_path = \"/\".join(\"\".join(video.split(\".\")[0:-1]).split(\"/\")[1:])\n",
    "        createFolder(f\"Data_Numpy/{video_path}\")\n",
    "        print(f\"Processing collection for... Data_Numpy/{video_path}\")\n",
    "        actual_video_frame = get_num_frame(video)\n",
    "        if actual_video_frame >= 75:\n",
    "            remind = actual_video_frame - 75\n",
    "            to_start = int(remind / 3)\n",
    "            j = 0\n",
    "            index = 0\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video)\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    while True:\n",
    "                        ret, frame = cap.read()\n",
    "                        j+=1\n",
    "                        if j < to_start or j%3 != to_start%3:\n",
    "                            continue\n",
    "                        \n",
    "                        index+=1\n",
    "                        if index > 25:\n",
    "                            break\n",
    "                            \n",
    "                        if ret == False:\n",
    "                            cap.release()\n",
    "                            break\n",
    "                            \n",
    "                            \n",
    "                        #Resize the frame\n",
    "                        resize_frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "                        # Make detections\n",
    "                        image, results = mediapipe_detection(resize_frame, holistic)\n",
    "\n",
    "                        # Drawing landmarks\n",
    "                        draw_styled_landmark(image, results)\n",
    "\n",
    "                        #Extracting keypoints\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        \n",
    "                        if not (keypoints==keypoints_blank).all():\n",
    "                            last_keypoints = keypoints\n",
    "                        else:\n",
    "                            index-=1\n",
    "                            continue\n",
    "                            \n",
    "                        #Saving data\n",
    "                        np.save(f\"Data_Numpy/{video_path}/{index}\", keypoints)\n",
    "                            \n",
    "                        cv2.imshow(video, image)\n",
    "                        if cv2.waitKey(10) and 0xFF == ord(\"q\"):\n",
    "                            break\n",
    "                            \n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "            finally:\n",
    "                while index <=25:\n",
    "                    np.save(f\"Data_Numpy/{video_path}/{index}\", last_keypoints)\n",
    "                    index+=1\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "        elif actual_video_frame >=50:\n",
    "            remind = actual_video_frame - 50\n",
    "            to_start = int(remind / 3)\n",
    "            j = 0\n",
    "            index = 0\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video)\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    while True:\n",
    "                        ret, frame = cap.read()\n",
    "                        j+=1\n",
    "\n",
    "                        if j < to_start or j%2 != to_start%2:\n",
    "                            continue\n",
    "\n",
    "                        index+=1\n",
    "                        if index > 25:\n",
    "                            break\n",
    "\n",
    "                        if ret == False:\n",
    "                            cap.release()\n",
    "                            break\n",
    "                        \n",
    "                        #Resize the frame\n",
    "                        resize_frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "                        # Make detections\n",
    "                        image, results = mediapipe_detection(resize_frame, holistic)\n",
    "\n",
    "                        # Drawing landmarks\n",
    "                        draw_styled_landmark(image, results)\n",
    "\n",
    "                        #Extracting keypoints\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        \n",
    "                        if not (keypoints==keypoints_blank).all():\n",
    "                            last_keypoints = keypoints\n",
    "                        else:\n",
    "                            index-=1\n",
    "                            continue\n",
    "                            \n",
    "                        #Saving data\n",
    "                        np.save(f\"Data_Numpy/{video_path}/{index}\", keypoints)\n",
    "\n",
    "                        cv2.imshow(video, image)\n",
    "                        if cv2.waitKey(10) and 0xFF == ord(\"q\"):\n",
    "                            break\n",
    "\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "            finally:\n",
    "                while index <=25:\n",
    "                    np.save(f\"Data_Numpy/{video_path}/{index}\", last_keypoints)\n",
    "                    index+=1\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "        elif actual_video_frame >=25:\n",
    "            remind = actual_video_frame - 25\n",
    "            to_start = int(remind / 3)\n",
    "            j = 0\n",
    "            index = 0\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video)\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    while True:\n",
    "                        ret, frame = cap.read()\n",
    "                        j+=1\n",
    "\n",
    "                        if j < to_start:\n",
    "                            continue\n",
    "\n",
    "                        index+=1\n",
    "                        if index > 25:\n",
    "                            break\n",
    "\n",
    "                        if ret == False:\n",
    "                            cap.release()\n",
    "                            break\n",
    "                            \n",
    "                        #Resize the frame\n",
    "                        resize_frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "                        # Make detections\n",
    "                        image, results = mediapipe_detection(resize_frame, holistic)\n",
    "\n",
    "                        # Drawing landmarks\n",
    "                        draw_styled_landmark(image, results)\n",
    "\n",
    "                        #Extracting keypoints\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        \n",
    "                        if not (keypoints==keypoints_blank).all():\n",
    "                            last_keypoints = keypoints\n",
    "                        else:\n",
    "                            index-=1\n",
    "                            continue\n",
    "                            \n",
    "                        #Saving data\n",
    "                        np.save(f\"Data_Numpy/{video_path}/{index}\", keypoints)\n",
    "\n",
    "                        cv2.imshow(video, image)\n",
    "                        if cv2.waitKey(10) and 0xFF == ord(\"q\"):\n",
    "                            break\n",
    "\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "            finally:\n",
    "                while index <=25:\n",
    "                    np.save(f\"Data_Numpy/{video_path}/{index}\", last_keypoints)\n",
    "                    index+=1\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "        else:\n",
    "            j = 0\n",
    "            index = 0\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video)\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    while True:\n",
    "                        ret, frame = cap.read()\n",
    "                        j+=1\n",
    "\n",
    "                        if j < to_start:\n",
    "                            continue\n",
    "\n",
    "                        index+=1\n",
    "                        if index > 25:\n",
    "                            break\n",
    "\n",
    "                        if ret == False:\n",
    "                            cap.release()\n",
    "                            break\n",
    "                            \n",
    "                        #Resize the frame\n",
    "                        resize_frame = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "                        # Make detections\n",
    "                        image, results = mediapipe_detection(resize_frame, holistic)\n",
    "\n",
    "                        # Drawing landmarks\n",
    "                        draw_styled_landmark(image, results)\n",
    "\n",
    "                        #Extracting keypoints\n",
    "                        keypoints = extract_keypoints(results)\n",
    "                        \n",
    "                        if not (keypoints==keypoints_blank).all():\n",
    "                            last_keypoints = keypoints\n",
    "                        else:\n",
    "                            index-=1\n",
    "                            continue\n",
    "                            \n",
    "                        #Saving data\n",
    "                        np.save(f\"Data_Numpy/{video_path}/{index}\", keypoints)\n",
    "\n",
    "                        cv2.imshow(video, image)\n",
    "                        last_frame = frame\n",
    "                        if cv2.waitKey(10) and 0xFF == ord(\"q\"):\n",
    "                            break\n",
    "\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "            finally:\n",
    "                while index <=25:\n",
    "                    np.save(f\"Data_Numpy/{video_path}/{index}\", last_keypoints)\n",
    "                    cv2.imshow(video, last_frame)\n",
    "                    index+=1\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a308315",
   "metadata": {},
   "source": [
    "### Setup data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2438a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, train_labels, test_sequences, test_labels = get_data(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f107b95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263, 25, 1662)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "523c7785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ce13d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 25, 1662)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fdc81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4f769d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9dda8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1263, 25, 1662)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7380b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(train_labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7862611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6a6ae",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe29fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c53e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"Logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80264579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(num_frame_per_video,1662)))\n",
    "#model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "#model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd634009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ca30d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 3s 114ms/step - loss: 4.6058 - categorical_accuracy: 0.0111\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 4.6040 - categorical_accuracy: 0.0143\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 4.6030 - categorical_accuracy: 0.0103\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 4.6009 - categorical_accuracy: 0.0150\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 4.5976 - categorical_accuracy: 0.0174\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 4.5918 - categorical_accuracy: 0.0150\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.5827 - categorical_accuracy: 0.0166\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 4.5797 - categorical_accuracy: 0.0198\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 4.5722 - categorical_accuracy: 0.0230\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 4.5677 - categorical_accuracy: 0.0190\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 4.5540 - categorical_accuracy: 0.0198\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 4.5402 - categorical_accuracy: 0.0245\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 4.5304 - categorical_accuracy: 0.0214\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 4.5238 - categorical_accuracy: 0.0245\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.5272 - categorical_accuracy: 0.0230\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 4.5141 - categorical_accuracy: 0.0293\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.4983 - categorical_accuracy: 0.0269\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.4721 - categorical_accuracy: 0.0277\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 4.4635 - categorical_accuracy: 0.0333\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 4.4567 - categorical_accuracy: 0.0325\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 4.4292 - categorical_accuracy: 0.0388\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 4.4041 - categorical_accuracy: 0.0325\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 4.4036 - categorical_accuracy: 0.0309\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.4062 - categorical_accuracy: 0.0380\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 4.3912 - categorical_accuracy: 0.0356\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.3632 - categorical_accuracy: 0.0428\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.3416 - categorical_accuracy: 0.0420\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.3048 - categorical_accuracy: 0.0483\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.2774 - categorical_accuracy: 0.0570\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.2426 - categorical_accuracy: 0.0578\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.2393 - categorical_accuracy: 0.0594\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 4.2140 - categorical_accuracy: 0.0689\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.1833 - categorical_accuracy: 0.0633\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 4.1397 - categorical_accuracy: 0.0673\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 2s 231ms/step - loss: 4.1337 - categorical_accuracy: 0.0728\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, batch_size=128,callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db3ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
